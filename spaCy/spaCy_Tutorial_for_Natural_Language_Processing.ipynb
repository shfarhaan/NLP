{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP71/qIIUyEUz+/ULp1msEB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shfarhaan/NLP/blob/main/spaCy/spaCy_Tutorial_for_Natural_Language_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **spaCy Tutorial for Natural Language Processing**"
      ],
      "metadata": {
        "id": "yUyQ0h_-NZGI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Introduction**\n",
        "\n",
        "Python has emerged as a popular language for Natural Language Processing (NLP) due to its simplicity and powerful libraries. One such library is spaCy, which provides easy-to-use and efficient tools for various NLP tasks. This tutorial aims to introduce beginners to spaCy and cover essential NLP tasks using this library.\n"
      ],
      "metadata": {
        "id": "AEgJAB0oNifw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introduction to spaCy"
      ],
      "metadata": {
        "id": "KeSI7f7pNuQ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What is spaCy?\n",
        "spaCy is an open-source library used for advanced NLP in Python. It is designed with the goal of being fast, streamlined, and simple to use. spaCy offers features for tokenization, named entity recognition (NER), part-of-speech tagging, dependency parsing, and more.\n"
      ],
      "metadata": {
        "id": "41OHgyEYN2kQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wk-klY1SN4jn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Installation\n",
        "To install spaCy, use pip:\n",
        "\n",
        "```bash\n",
        "pip install spacy\n",
        "```"
      ],
      "metadata": {
        "id": "xP6z0YqQNuNg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy"
      ],
      "metadata": {
        "id": "k5l68-ghN-ET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the spaCy NLP object\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Preprocess the text\n",
        "text = \"This is a sample text.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "# Print the tokens\n",
        "for token in doc:\n",
        "    print(token.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bi7cKONaOCRf",
        "outputId": "d1c0d382-5241-4562-a725-67adf761cc68"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This\n",
            "is\n",
            "a\n",
            "sample\n",
            "text\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Replace `en_core_web_sm` with the language model you want to download. This example uses the English language model."
      ],
      "metadata": {
        "id": "FaGCKLpQOn85"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Text Preprocessing with spaCy"
      ],
      "metadata": {
        "id": "1le_3BnHNuKG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tokenization\n",
        "Tokenization breaks text into individual words or tokens. Here's how to tokenize text using spaCy:\n",
        "\n",
        "```python\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "text = \"Tokenization breaks text into tokens.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "for token in doc:\n",
        "    print(token.text)\n",
        "```\n"
      ],
      "metadata": {
        "id": "wY0mwtmoOw8i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "text = \"Tokenization breaks text into tokens.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "for token in doc:\n",
        "    print(token.text)"
      ],
      "metadata": {
        "id": "mOjObAlvOxor",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40e042ed-b664-4bff-937d-ab409915b1d2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenization\n",
            "breaks\n",
            "text\n",
            "into\n",
            "tokens\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Lemmatization\n",
        "Lemmatization reduces words to their base or root form. Here's an example:\n",
        "\n",
        "```python\n",
        "for token in doc:\n",
        "    print(token.text, token.lemma_)\n",
        "```\n",
        "\n",
        "#### Part-of-Speech Tagging\n",
        "Identifying the grammatical parts of a sentence using spaCy:\n",
        "\n",
        "```python\n",
        "for token in doc:\n",
        "    print(token.text, token.pos_)\n",
        "```"
      ],
      "metadata": {
        "id": "cB913kAdNuGc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the spaCy library\n",
        "import spacy\n",
        "\n",
        "# Load the English language model \"en_core_web_sm\"\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Define the input text\n",
        "text = \"Tokenization breaks text into tokens.\"\n",
        "\n",
        "# Process the text using the spaCy model\n",
        "doc = nlp(text)\n",
        "\n",
        "# # Iterate over each token in the processed document and its index\n",
        "# for i, token in enumerate(doc):\n",
        "#     # Print the token's text, with 15 characters of space, followed by its index with 2 characters of space\n",
        "#     print(f\"{token.text:15} - is the {i:2}th token from the text\")\n",
        "\n",
        "\n",
        "# for i, token in enumerate(doc):\n",
        "#     print(f\"{token.text:15} is the {i:2}th token and after lemmatizing it is - {token.lemma_:2}\" )\n",
        "\n",
        "for i, token in enumerate(doc):\n",
        "    print(f\"{token.text:15} is the {i:2}th token and after lemmatizing it is - {token.pos_:2}\" )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NRbgjFBL8gk",
        "outputId": "987c0c1a-99a4-485c-c200-0b13364958e2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenization    is the  0th token and after lemmatizing it is - NOUN\n",
            "breaks          is the  1th token and after lemmatizing it is - VERB\n",
            "text            is the  2th token and after lemmatizing it is - NOUN\n",
            "into            is the  3th token and after lemmatizing it is - ADP\n",
            "tokens          is the  4th token and after lemmatizing it is - NOUN\n",
            ".               is the  5th token and after lemmatizing it is - PUNCT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jSV5_sFqO1ap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Named Entity Recognition (NER)\n",
        "\n",
        "Named Entity Recognition identifies entities in text, such as names, organizations, locations, etc. Example:\n",
        "\n",
        "```python\n",
        "text = \"Apple is situated in California.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)\n",
        "```"
      ],
      "metadata": {
        "id": "L6o9fwaUNuC-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yI7jIPYqO6Oo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Dependency Parsing\n",
        "\n",
        "Dependency Parsing reveals the grammatical structure of a sentence. Example:\n",
        "\n",
        "```python\n",
        "for token in doc:\n",
        "    print(token.text, token.dep_, token.head.text, token.head.pos_)\n",
        "```\n"
      ],
      "metadata": {
        "id": "Qpr_6Z9tNt_f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the spaCy library\n",
        "import spacy\n",
        "\n",
        "# Load the English language model \"en_core_web_sm\"\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Define the input text\n",
        "text = \"Tokenization breaks text into tokens.\"\n",
        "\n",
        "# Process the text using the spaCy model\n",
        "doc = nlp(text)\n",
        "\n",
        "# Iterate over each token in the processed document\n",
        "for token in doc:\n",
        "    # Print the token's text\n",
        "    token_text = token.text\n",
        "\n",
        "    # Print the token's dependency label\n",
        "    dependency_label = token.dep_\n",
        "\n",
        "    # Print the text of the token's head\n",
        "    head_token_text = token.head.text\n",
        "\n",
        "    # Print the part of speech of the token's head\n",
        "    head_token_pos = token.head.pos_\n",
        "\n",
        "    # Combine and print all the information\n",
        "    print(f\"Token Text: {token_text}, Dependency Label: {dependency_label}, Head Token Text: {head_token_text}, Head Token POS: {head_token_pos}\")\n"
      ],
      "metadata": {
        "id": "w77mAdGJO8_9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dc35171-00d0-4ae1-e4ad-a1db02c50ee1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token Text: Tokenization, Dependency Label: nsubj, Head Token Text: breaks, Head Token POS: VERB\n",
            "Token Text: breaks, Dependency Label: ROOT, Head Token Text: breaks, Head Token POS: VERB\n",
            "Token Text: text, Dependency Label: dobj, Head Token Text: breaks, Head Token POS: VERB\n",
            "Token Text: into, Dependency Label: prep, Head Token Text: breaks, Head Token POS: VERB\n",
            "Token Text: tokens, Dependency Label: pobj, Head Token Text: into, Head Token POS: ADP\n",
            "Token Text: ., Dependency Label: punct, Head Token Text: breaks, Head Token POS: VERB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 9. Text Classification with spaCy\n",
        "\n",
        "Text classification categorizes text into predefined classes or categories. Here's a simple example:\n",
        "\n",
        "```python\n",
        "# Training data preparation\n",
        "train_texts = [\"Text 1\", \"Text 2\", \"Text 3\"]\n",
        "train_labels = [\"Label 1\", \"Label 2\", \"Label 3\"]\n",
        "\n",
        "# Train a text classification model\n",
        "textcat = nlp.create_pipe(\"textcat\")\n",
        "nlp.add_pipe(textcat, last=True)\n",
        "\n",
        "textcat.add_label(\"Label 1\")\n",
        "textcat.add_label(\"Label 2\")\n",
        "textcat.add_label(\"Label 3\")\n",
        "\n",
        "train_data = list(zip(train_texts, [{\"cats\": {label: 1.0 if label == true_label else 0.0 for label in train_labels}} for true_label in train_labels]))\n",
        "\n",
        "for text, annotations in train_data:\n",
        "    doc = nlp.make_doc(text)\n",
        "    example = Example.from_dict(doc, annotations)\n",
        "    nlp.update([example], losses={textcat: losses.CategoricalCrossentropy()})\n",
        "\n",
        "# Classify new text\n",
        "new_text = \"New text to classify\"\n",
        "doc = nlp(new_text)\n",
        "print(doc.cats)\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "ta8LES_cO_xn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training data preparation\n",
        "train_texts = [\"Text 1\", \"Text 2\", \"Text 3\"]\n",
        "train_labels = [\"Label 1\", \"Label 2\", \"Label 3\"]\n",
        "\n",
        "# Train a text classification model\n",
        "textcat = nlp.create_pipe(\"textcat\")\n",
        "nlp.add_pipe(textcat, last=True)\n",
        "\n",
        "textcat.add_label(\"Label 1\")\n",
        "textcat.add_label(\"Label 2\")\n",
        "textcat.add_label(\"Label 3\")\n",
        "\n",
        "train_data = list(zip(train_texts, [{\"cats\": {label: 1.0 if label == true_label else 0.0 for label in train_labels}} for true_label in train_labels]))\n",
        "\n",
        "for text, annotations in train_data:\n",
        "    doc = nlp.make_doc(text)\n",
        "    example = Example.from_dict(doc, annotations)\n",
        "    nlp.update([example], losses={textcat: losses.CategoricalCrossentropy()})\n",
        "\n",
        "# Classify new text\n",
        "new_text = \"New text to classify\"\n",
        "doc = nlp(new_text)\n",
        "print(doc.cats)"
      ],
      "metadata": {
        "id": "WUUJLiNuPArX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "outputId": "df4e20f2-c625-4277-f2d4-c2f720d42687"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-59ae8ae1f272>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Train a text classification model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtextcat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"textcat\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtextcat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtextcat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Label 1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36madd_pipe\u001b[0;34m(self, factory_name, name, before, after, first, last, source, config, raw_config, validate)\u001b[0m\n\u001b[1;32m    798\u001b[0m             \u001b[0mbad_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfactory_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m             \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE966\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbad_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 800\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    801\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfactory_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponent_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: [E966] `nlp.add_pipe` now takes the string name of the registered component factory, not a callable component. Expected string, but got <spacy.pipeline.textcat.TextCategorizer object at 0x7a428f8e7e20> (name: 'None').\n\n- If you created your component with `nlp.create_pipe('name')`: remove nlp.create_pipe and call `nlp.add_pipe('name')` instead.\n\n- If you passed in a component like `TextCategorizer()`: call `nlp.add_pipe` with the string name instead, e.g. `nlp.add_pipe('textcat')`.\n\n- If you're using a custom component: Add the decorator `@Language.component` (for function components) or `@Language.factory` (for class components / factories) to your custom component and assign it a name, e.g. `@Language.component('your_name')`. You can then run `nlp.add_pipe('your_name')` to add it to the pipeline."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10. Practical Examples and Projects\n",
        "\n",
        "#### Project 1: Sentiment Analysis\n",
        "Perform sentiment analysis on a dataset using spaCy for text classification.\n",
        "\n",
        "#### Project 2: Information Extraction\n",
        "Extract specific information, like dates or quantities, from a set of documents using spaCy.\n"
      ],
      "metadata": {
        "id": "BLrF53y9PCYP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### 11. Conclusion\n",
        "\n",
        "In this tutorial, we covered the basics of Python and spaCy for NLP tasks. We explored text preprocessing, named entity recognition, dependency parsing, text classification, and presented practical examples and projects. To further advance your understanding, continue exploring spaCy's documentation, practice on different datasets, and engage in real-world NLP projects. With consistent practice, you'll become proficient in NLP using Python and spaCy.\n",
        "\n",
        "Remember, NLP is a vast field, and this tutorial only scratches the surface. Continual learning and hands-on experience will enhance your skills and understanding.\n",
        "\n",
        "I hope this tutorial serves as a solid foundation for your journey into NLP with spaCy and Python."
      ],
      "metadata": {
        "id": "WuDLmScvNVcZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqROeqtnNTHN"
      },
      "outputs": [],
      "source": []
    }
  ]
}